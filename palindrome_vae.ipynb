{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b13e45fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6824c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic VAE \n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, latent_dim=10):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(hidden_dim // 2, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim // 2, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon_x = self.decode(z)\n",
    "        return recon_x, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f846d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    bce = nn.BCELoss(reduction='sum')(recon_x, x)\n",
    "    kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return bce + kld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f701788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_palindromic_binary_data(num_samples=1000, length=20):\n",
    "    data = []\n",
    "    for _ in range(num_samples):\n",
    "        half = np.random.randint(0, 2, size=length // 2)\n",
    "        palindrome = np.concatenate((half, half[::-1]))\n",
    "        data.append(palindrome)\n",
    "    return torch.tensor(data, dtype=torch.float32)\n",
    "def is_palindrome(seq):\n",
    "    return seq == seq[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99b65af",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_palindromes = generate_palindromic_binary_data()\n",
    "dataloader = DataLoader(TensorDataset(binary_palindromes), batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597d5308",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE()\n",
    "optimizer = optim.Adam(vae.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59737b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 13.9530\n",
      "Epoch 2, Loss: 13.8792\n",
      "Epoch 3, Loss: 13.8661\n",
      "Epoch 4, Loss: 13.8645\n",
      "Epoch 5, Loss: 13.8622\n",
      "Epoch 6, Loss: 13.8670\n",
      "Epoch 7, Loss: 13.8561\n",
      "Epoch 8, Loss: 13.8602\n",
      "Epoch 9, Loss: 13.8677\n",
      "Epoch 10, Loss: 13.8582\n",
      "Epoch 11, Loss: 13.8626\n",
      "Epoch 12, Loss: 13.8572\n",
      "Epoch 13, Loss: 13.8595\n",
      "Epoch 14, Loss: 13.8573\n",
      "Epoch 15, Loss: 13.8539\n",
      "Epoch 16, Loss: 13.8624\n",
      "Epoch 17, Loss: 13.8426\n",
      "Epoch 18, Loss: 13.8567\n",
      "Epoch 19, Loss: 13.8437\n",
      "Epoch 20, Loss: 13.8373\n",
      "Epoch 21, Loss: 13.8361\n",
      "Epoch 22, Loss: 13.8158\n",
      "Epoch 23, Loss: 13.8227\n",
      "Epoch 24, Loss: 13.8188\n",
      "Epoch 25, Loss: 13.7771\n",
      "Epoch 26, Loss: 13.7810\n",
      "Epoch 27, Loss: 13.7657\n",
      "Epoch 28, Loss: 13.7144\n",
      "Epoch 29, Loss: 13.7247\n",
      "Epoch 30, Loss: 13.7023\n",
      "Epoch 31, Loss: 13.6464\n",
      "Epoch 32, Loss: 13.5765\n",
      "Epoch 33, Loss: 13.6254\n",
      "Epoch 34, Loss: 13.6524\n",
      "Epoch 35, Loss: 13.6120\n",
      "Epoch 36, Loss: 13.5025\n",
      "Epoch 37, Loss: 13.5364\n",
      "Epoch 38, Loss: 13.4724\n",
      "Epoch 39, Loss: 13.5550\n",
      "Epoch 40, Loss: 13.4663\n",
      "Epoch 41, Loss: 13.4867\n",
      "Epoch 42, Loss: 13.5668\n",
      "Epoch 43, Loss: 13.4092\n",
      "Epoch 44, Loss: 13.4327\n",
      "Epoch 45, Loss: 13.4570\n",
      "Epoch 46, Loss: 13.4059\n",
      "Epoch 47, Loss: 13.3419\n",
      "Epoch 48, Loss: 13.2157\n",
      "Epoch 49, Loss: 13.2180\n",
      "Epoch 50, Loss: 13.2637\n",
      "Epoch 51, Loss: 13.2105\n",
      "Epoch 52, Loss: 13.1982\n",
      "Epoch 53, Loss: 13.2424\n",
      "Epoch 54, Loss: 13.2218\n",
      "Epoch 55, Loss: 13.1784\n",
      "Epoch 56, Loss: 13.0150\n",
      "Epoch 57, Loss: 13.1024\n",
      "Epoch 58, Loss: 13.1113\n",
      "Epoch 59, Loss: 13.0532\n",
      "Epoch 60, Loss: 13.1469\n",
      "Epoch 61, Loss: 12.9927\n",
      "Epoch 62, Loss: 12.8770\n",
      "Epoch 63, Loss: 12.9901\n",
      "Epoch 64, Loss: 12.8930\n",
      "Epoch 65, Loss: 12.8267\n",
      "Epoch 66, Loss: 12.7594\n",
      "Epoch 67, Loss: 12.7524\n",
      "Epoch 68, Loss: 12.6876\n",
      "Epoch 69, Loss: 12.6970\n",
      "Epoch 70, Loss: 12.5550\n",
      "Epoch 71, Loss: 12.6904\n",
      "Epoch 72, Loss: 12.4632\n",
      "Epoch 73, Loss: 12.5728\n",
      "Epoch 74, Loss: 12.4846\n",
      "Epoch 75, Loss: 12.3413\n",
      "Epoch 76, Loss: 12.3316\n",
      "Epoch 77, Loss: 12.2163\n",
      "Epoch 78, Loss: 12.2562\n",
      "Epoch 79, Loss: 12.1991\n",
      "Epoch 80, Loss: 12.0751\n",
      "Epoch 81, Loss: 12.2023\n",
      "Epoch 82, Loss: 12.0505\n",
      "Epoch 83, Loss: 12.0087\n",
      "Epoch 84, Loss: 12.0440\n",
      "Epoch 85, Loss: 11.9079\n",
      "Epoch 86, Loss: 11.9412\n",
      "Epoch 87, Loss: 11.8656\n",
      "Epoch 88, Loss: 11.9493\n",
      "Epoch 89, Loss: 11.7169\n",
      "Epoch 90, Loss: 11.7708\n",
      "Epoch 91, Loss: 11.8692\n",
      "Epoch 92, Loss: 11.7563\n",
      "Epoch 93, Loss: 11.7522\n",
      "Epoch 94, Loss: 11.8584\n",
      "Epoch 95, Loss: 11.7866\n",
      "Epoch 96, Loss: 11.6530\n",
      "Epoch 97, Loss: 11.6474\n",
      "Epoch 98, Loss: 11.6440\n",
      "Epoch 99, Loss: 11.6321\n",
      "Epoch 100, Loss: 11.6775\n"
     ]
    }
   ],
   "source": [
    "def train(epochs=100):\n",
    "    vae.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            x = batch[0]\n",
    "            optimizer.zero_grad()\n",
    "            recon_x, mu, logvar = vae(x)\n",
    "            loss = loss_function(recon_x, x, mu, logvar)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(dataloader.dataset):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e0dfd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 11.5439\n",
      "Epoch 2, Loss: 11.5542\n",
      "Epoch 3, Loss: 11.5475\n",
      "Epoch 4, Loss: 11.6176\n",
      "Epoch 5, Loss: 11.6832\n",
      "Epoch 6, Loss: 11.4448\n",
      "Epoch 7, Loss: 11.4635\n",
      "Epoch 8, Loss: 11.6732\n",
      "Epoch 9, Loss: 11.6002\n",
      "Epoch 10, Loss: 11.5491\n",
      "Epoch 11, Loss: 11.5418\n",
      "Epoch 12, Loss: 11.4843\n",
      "Epoch 13, Loss: 11.5603\n",
      "Epoch 14, Loss: 11.5840\n",
      "Epoch 15, Loss: 11.5080\n",
      "Epoch 16, Loss: 11.5318\n",
      "Epoch 17, Loss: 11.5162\n",
      "Epoch 18, Loss: 11.5105\n",
      "Epoch 19, Loss: 11.6413\n",
      "Epoch 20, Loss: 11.5528\n",
      "Epoch 21, Loss: 11.5645\n",
      "Epoch 22, Loss: 11.6402\n",
      "Epoch 23, Loss: 11.6034\n",
      "Epoch 24, Loss: 11.4565\n",
      "Epoch 25, Loss: 11.5290\n",
      "Epoch 26, Loss: 11.5416\n",
      "Epoch 27, Loss: 11.5858\n",
      "Epoch 28, Loss: 11.5257\n",
      "Epoch 29, Loss: 11.5250\n",
      "Epoch 30, Loss: 11.4942\n",
      "Epoch 31, Loss: 11.5171\n",
      "Epoch 32, Loss: 11.5146\n",
      "Epoch 33, Loss: 11.4891\n",
      "Epoch 34, Loss: 11.6004\n",
      "Epoch 35, Loss: 11.4472\n",
      "Epoch 36, Loss: 11.5298\n",
      "Epoch 37, Loss: 11.4153\n",
      "Epoch 38, Loss: 11.6248\n",
      "Epoch 39, Loss: 11.5264\n",
      "Epoch 40, Loss: 11.4928\n",
      "Epoch 41, Loss: 11.4722\n",
      "Epoch 42, Loss: 11.4540\n",
      "Epoch 43, Loss: 11.4501\n",
      "Epoch 44, Loss: 11.5330\n",
      "Epoch 45, Loss: 11.4817\n",
      "Epoch 46, Loss: 11.5072\n",
      "Epoch 47, Loss: 11.5413\n",
      "Epoch 48, Loss: 11.3992\n",
      "Epoch 49, Loss: 11.4930\n",
      "Epoch 50, Loss: 11.5721\n",
      "Epoch 51, Loss: 11.5078\n",
      "Epoch 52, Loss: 11.4968\n",
      "Epoch 53, Loss: 11.3677\n",
      "Epoch 54, Loss: 11.5044\n",
      "Epoch 55, Loss: 11.5379\n",
      "Epoch 56, Loss: 11.5191\n",
      "Epoch 57, Loss: 11.4955\n",
      "Epoch 58, Loss: 11.4990\n",
      "Epoch 59, Loss: 11.5815\n",
      "Epoch 60, Loss: 11.6560\n",
      "Epoch 61, Loss: 11.4074\n",
      "Epoch 62, Loss: 11.5513\n",
      "Epoch 63, Loss: 11.5182\n",
      "Epoch 64, Loss: 11.5600\n",
      "Epoch 65, Loss: 11.4376\n",
      "Epoch 66, Loss: 11.6226\n",
      "Epoch 67, Loss: 11.3647\n",
      "Epoch 68, Loss: 11.4414\n",
      "Epoch 69, Loss: 11.4788\n",
      "Epoch 70, Loss: 11.4690\n",
      "Epoch 71, Loss: 11.4725\n",
      "Epoch 72, Loss: 11.5120\n",
      "Epoch 73, Loss: 11.4626\n",
      "Epoch 74, Loss: 11.5020\n",
      "Epoch 75, Loss: 11.4482\n",
      "Epoch 76, Loss: 11.4644\n",
      "Epoch 77, Loss: 11.4103\n",
      "Epoch 78, Loss: 11.4591\n",
      "Epoch 79, Loss: 11.4452\n",
      "Epoch 80, Loss: 11.4897\n",
      "Epoch 81, Loss: 11.3850\n",
      "Epoch 82, Loss: 11.4412\n",
      "Epoch 83, Loss: 11.4271\n",
      "Epoch 84, Loss: 11.4891\n",
      "Epoch 85, Loss: 11.4535\n",
      "Epoch 86, Loss: 11.4404\n",
      "Epoch 87, Loss: 11.4910\n",
      "Epoch 88, Loss: 11.4511\n",
      "Epoch 89, Loss: 11.4624\n",
      "Epoch 90, Loss: 11.3265\n",
      "Epoch 91, Loss: 11.4042\n",
      "Epoch 92, Loss: 11.4140\n",
      "Epoch 93, Loss: 11.4422\n",
      "Epoch 94, Loss: 11.4136\n",
      "Epoch 95, Loss: 11.4765\n",
      "Epoch 96, Loss: 11.5121\n",
      "Epoch 97, Loss: 11.5600\n",
      "Epoch 98, Loss: 11.4001\n",
      "Epoch 99, Loss: 11.4873\n",
      "Epoch 100, Loss: 11.5202\n",
      "Epoch 101, Loss: 11.3716\n",
      "Epoch 102, Loss: 11.4223\n",
      "Epoch 103, Loss: 11.3718\n",
      "Epoch 104, Loss: 11.3252\n",
      "Epoch 105, Loss: 11.4040\n",
      "Epoch 106, Loss: 11.2911\n",
      "Epoch 107, Loss: 11.3522\n",
      "Epoch 108, Loss: 11.3195\n",
      "Epoch 109, Loss: 11.4438\n",
      "Epoch 110, Loss: 11.4587\n",
      "Epoch 111, Loss: 11.4326\n",
      "Epoch 112, Loss: 11.4291\n",
      "Epoch 113, Loss: 11.3419\n",
      "Epoch 114, Loss: 11.3264\n",
      "Epoch 115, Loss: 11.4577\n",
      "Epoch 116, Loss: 11.4030\n",
      "Epoch 117, Loss: 11.3934\n",
      "Epoch 118, Loss: 11.2912\n",
      "Epoch 119, Loss: 11.3895\n",
      "Epoch 120, Loss: 11.2377\n",
      "Epoch 121, Loss: 11.2547\n",
      "Epoch 122, Loss: 11.4319\n",
      "Epoch 123, Loss: 11.3924\n",
      "Epoch 124, Loss: 11.3623\n",
      "Epoch 125, Loss: 11.4216\n",
      "Epoch 126, Loss: 11.3792\n",
      "Epoch 127, Loss: 11.3823\n",
      "Epoch 128, Loss: 11.3140\n",
      "Epoch 129, Loss: 11.3260\n",
      "Epoch 130, Loss: 11.2556\n",
      "Epoch 131, Loss: 11.3473\n",
      "Epoch 132, Loss: 11.2948\n",
      "Epoch 133, Loss: 11.1362\n",
      "Epoch 134, Loss: 11.2520\n",
      "Epoch 135, Loss: 11.3279\n",
      "Epoch 136, Loss: 11.2580\n",
      "Epoch 137, Loss: 11.2441\n",
      "Epoch 138, Loss: 11.2154\n",
      "Epoch 139, Loss: 11.2522\n",
      "Epoch 140, Loss: 11.1825\n",
      "Epoch 141, Loss: 11.2205\n",
      "Epoch 142, Loss: 11.2968\n",
      "Epoch 143, Loss: 11.2982\n",
      "Epoch 144, Loss: 11.4857\n",
      "Epoch 145, Loss: 11.2706\n",
      "Epoch 146, Loss: 11.3113\n",
      "Epoch 147, Loss: 11.1602\n",
      "Epoch 148, Loss: 11.2138\n",
      "Epoch 149, Loss: 11.1556\n",
      "Epoch 150, Loss: 11.1246\n",
      "Epoch 151, Loss: 11.2463\n",
      "Epoch 152, Loss: 11.3143\n",
      "Epoch 153, Loss: 11.2158\n",
      "Epoch 154, Loss: 11.1831\n",
      "Epoch 155, Loss: 11.2452\n",
      "Epoch 156, Loss: 11.2479\n",
      "Epoch 157, Loss: 11.1796\n",
      "Epoch 158, Loss: 11.2610\n",
      "Epoch 159, Loss: 11.2206\n",
      "Epoch 160, Loss: 11.1542\n",
      "Epoch 161, Loss: 11.1075\n",
      "Epoch 162, Loss: 11.0383\n",
      "Epoch 163, Loss: 11.2402\n",
      "Epoch 164, Loss: 11.0074\n",
      "Epoch 165, Loss: 11.1756\n",
      "Epoch 166, Loss: 11.1959\n",
      "Epoch 167, Loss: 11.0003\n",
      "Epoch 168, Loss: 11.1863\n",
      "Epoch 169, Loss: 11.1581\n",
      "Epoch 170, Loss: 11.1024\n",
      "Epoch 171, Loss: 11.1097\n",
      "Epoch 172, Loss: 11.2106\n",
      "Epoch 173, Loss: 11.1043\n",
      "Epoch 174, Loss: 11.1030\n",
      "Epoch 175, Loss: 11.0144\n",
      "Epoch 176, Loss: 11.0905\n",
      "Epoch 177, Loss: 10.9986\n",
      "Epoch 178, Loss: 11.0873\n",
      "Epoch 179, Loss: 11.1288\n",
      "Epoch 180, Loss: 10.9686\n",
      "Epoch 181, Loss: 11.1376\n",
      "Epoch 182, Loss: 11.1005\n",
      "Epoch 183, Loss: 11.0458\n",
      "Epoch 184, Loss: 11.0403\n",
      "Epoch 185, Loss: 11.1087\n",
      "Epoch 186, Loss: 11.1057\n",
      "Epoch 187, Loss: 11.1123\n",
      "Epoch 188, Loss: 11.0088\n",
      "Epoch 189, Loss: 10.9917\n",
      "Epoch 190, Loss: 10.9935\n",
      "Epoch 191, Loss: 10.9835\n",
      "Epoch 192, Loss: 10.9292\n",
      "Epoch 193, Loss: 10.9335\n",
      "Epoch 194, Loss: 10.9491\n",
      "Epoch 195, Loss: 11.0518\n",
      "Epoch 196, Loss: 10.8494\n",
      "Epoch 197, Loss: 10.9852\n",
      "Epoch 198, Loss: 10.9722\n",
      "Epoch 199, Loss: 10.9285\n",
      "Epoch 200, Loss: 11.0111\n",
      "Epoch 201, Loss: 10.9034\n",
      "Epoch 202, Loss: 11.0097\n",
      "Epoch 203, Loss: 10.9934\n",
      "Epoch 204, Loss: 11.0906\n",
      "Epoch 205, Loss: 10.9347\n",
      "Epoch 206, Loss: 11.0017\n",
      "Epoch 207, Loss: 11.0535\n",
      "Epoch 208, Loss: 10.8851\n",
      "Epoch 209, Loss: 10.8997\n",
      "Epoch 210, Loss: 10.8581\n",
      "Epoch 211, Loss: 10.9916\n",
      "Epoch 212, Loss: 10.8491\n",
      "Epoch 213, Loss: 10.9478\n",
      "Epoch 214, Loss: 10.9028\n",
      "Epoch 215, Loss: 10.7816\n",
      "Epoch 216, Loss: 10.8880\n",
      "Epoch 217, Loss: 10.8279\n",
      "Epoch 218, Loss: 10.9727\n",
      "Epoch 219, Loss: 10.8592\n",
      "Epoch 220, Loss: 10.9142\n",
      "Epoch 221, Loss: 10.8127\n",
      "Epoch 222, Loss: 10.8834\n",
      "Epoch 223, Loss: 10.7921\n",
      "Epoch 224, Loss: 10.7725\n",
      "Epoch 225, Loss: 10.7961\n",
      "Epoch 226, Loss: 10.8499\n",
      "Epoch 227, Loss: 10.7569\n",
      "Epoch 228, Loss: 10.7989\n",
      "Epoch 229, Loss: 10.7118\n",
      "Epoch 230, Loss: 10.7778\n",
      "Epoch 231, Loss: 10.6502\n",
      "Epoch 232, Loss: 10.8044\n",
      "Epoch 233, Loss: 10.7503\n",
      "Epoch 234, Loss: 10.7208\n",
      "Epoch 235, Loss: 10.6648\n",
      "Epoch 236, Loss: 10.8070\n",
      "Epoch 237, Loss: 10.6445\n",
      "Epoch 238, Loss: 10.6621\n",
      "Epoch 239, Loss: 10.6115\n",
      "Epoch 240, Loss: 10.6387\n",
      "Epoch 241, Loss: 10.6671\n",
      "Epoch 242, Loss: 10.6109\n",
      "Epoch 243, Loss: 10.6646\n",
      "Epoch 244, Loss: 10.5863\n",
      "Epoch 245, Loss: 10.5366\n",
      "Epoch 246, Loss: 10.5731\n",
      "Epoch 247, Loss: 10.6733\n",
      "Epoch 248, Loss: 10.5019\n",
      "Epoch 249, Loss: 10.4675\n",
      "Epoch 250, Loss: 10.5188\n",
      "Epoch 251, Loss: 10.5938\n",
      "Epoch 252, Loss: 10.3654\n",
      "Epoch 253, Loss: 10.4952\n",
      "Epoch 254, Loss: 10.4304\n",
      "Epoch 255, Loss: 10.4309\n",
      "Epoch 256, Loss: 10.4294\n",
      "Epoch 257, Loss: 10.4751\n",
      "Epoch 258, Loss: 10.5361\n",
      "Epoch 259, Loss: 10.3740\n",
      "Epoch 260, Loss: 10.2972\n",
      "Epoch 261, Loss: 10.4100\n",
      "Epoch 262, Loss: 10.3518\n",
      "Epoch 263, Loss: 10.3596\n",
      "Epoch 264, Loss: 10.2802\n",
      "Epoch 265, Loss: 10.4882\n",
      "Epoch 266, Loss: 10.2565\n",
      "Epoch 267, Loss: 10.2570\n",
      "Epoch 268, Loss: 10.2788\n",
      "Epoch 269, Loss: 10.2703\n",
      "Epoch 270, Loss: 10.1784\n",
      "Epoch 271, Loss: 10.2015\n",
      "Epoch 272, Loss: 10.3081\n",
      "Epoch 273, Loss: 10.2568\n",
      "Epoch 274, Loss: 10.2667\n",
      "Epoch 275, Loss: 10.0907\n",
      "Epoch 276, Loss: 10.2287\n",
      "Epoch 277, Loss: 10.0756\n",
      "Epoch 278, Loss: 10.1278\n",
      "Epoch 279, Loss: 10.1663\n",
      "Epoch 280, Loss: 10.0664\n",
      "Epoch 281, Loss: 10.1258\n",
      "Epoch 282, Loss: 10.2314\n",
      "Epoch 283, Loss: 10.1593\n",
      "Epoch 284, Loss: 10.1741\n",
      "Epoch 285, Loss: 9.9343\n",
      "Epoch 286, Loss: 10.0407\n",
      "Epoch 287, Loss: 10.0637\n",
      "Epoch 288, Loss: 10.0329\n",
      "Epoch 289, Loss: 10.0918\n",
      "Epoch 290, Loss: 9.9591\n",
      "Epoch 291, Loss: 9.9419\n",
      "Epoch 292, Loss: 10.0688\n",
      "Epoch 293, Loss: 10.0849\n",
      "Epoch 294, Loss: 9.8181\n",
      "Epoch 295, Loss: 9.9413\n",
      "Epoch 296, Loss: 9.9505\n",
      "Epoch 297, Loss: 10.0682\n",
      "Epoch 298, Loss: 9.8633\n",
      "Epoch 299, Loss: 9.9370\n",
      "Epoch 300, Loss: 9.9492\n",
      "Epoch 301, Loss: 9.9291\n",
      "Epoch 302, Loss: 9.8261\n",
      "Epoch 303, Loss: 10.0047\n",
      "Epoch 304, Loss: 9.9461\n",
      "Epoch 305, Loss: 9.8154\n",
      "Epoch 306, Loss: 9.8408\n",
      "Epoch 307, Loss: 9.7740\n",
      "Epoch 308, Loss: 9.7746\n",
      "Epoch 309, Loss: 9.7981\n",
      "Epoch 310, Loss: 9.8520\n",
      "Epoch 311, Loss: 9.7614\n",
      "Epoch 312, Loss: 9.7846\n",
      "Epoch 313, Loss: 9.8424\n",
      "Epoch 314, Loss: 9.8195\n",
      "Epoch 315, Loss: 9.7997\n",
      "Epoch 316, Loss: 9.8005\n",
      "Epoch 317, Loss: 9.8613\n",
      "Epoch 318, Loss: 9.6880\n",
      "Epoch 319, Loss: 9.7883\n",
      "Epoch 320, Loss: 9.7545\n",
      "Epoch 321, Loss: 9.7234\n",
      "Epoch 322, Loss: 9.7405\n",
      "Epoch 323, Loss: 9.8802\n",
      "Epoch 324, Loss: 9.7368\n",
      "Epoch 325, Loss: 9.8615\n",
      "Epoch 326, Loss: 9.6452\n",
      "Epoch 327, Loss: 9.6604\n",
      "Epoch 328, Loss: 9.7968\n",
      "Epoch 329, Loss: 9.5420\n",
      "Epoch 330, Loss: 9.6643\n",
      "Epoch 331, Loss: 9.6513\n",
      "Epoch 332, Loss: 9.7380\n",
      "Epoch 333, Loss: 9.7579\n",
      "Epoch 334, Loss: 9.5810\n",
      "Epoch 335, Loss: 9.8073\n",
      "Epoch 336, Loss: 9.7070\n",
      "Epoch 337, Loss: 9.6441\n",
      "Epoch 338, Loss: 9.6175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 339, Loss: 9.6978\n",
      "Epoch 340, Loss: 9.5877\n",
      "Epoch 341, Loss: 9.6999\n",
      "Epoch 342, Loss: 9.7238\n",
      "Epoch 343, Loss: 9.7009\n",
      "Epoch 344, Loss: 9.6558\n",
      "Epoch 345, Loss: 9.6969\n",
      "Epoch 346, Loss: 9.5256\n",
      "Epoch 347, Loss: 9.5845\n",
      "Epoch 348, Loss: 9.4831\n",
      "Epoch 349, Loss: 9.5867\n",
      "Epoch 350, Loss: 9.4922\n",
      "Epoch 351, Loss: 9.5940\n",
      "Epoch 352, Loss: 9.5156\n",
      "Epoch 353, Loss: 9.5144\n",
      "Epoch 354, Loss: 9.5949\n",
      "Epoch 355, Loss: 9.5940\n",
      "Epoch 356, Loss: 9.5393\n",
      "Epoch 357, Loss: 9.5097\n",
      "Epoch 358, Loss: 9.4882\n",
      "Epoch 359, Loss: 9.4992\n",
      "Epoch 360, Loss: 9.6694\n",
      "Epoch 361, Loss: 9.5666\n",
      "Epoch 362, Loss: 9.7904\n",
      "Epoch 363, Loss: 9.6879\n",
      "Epoch 364, Loss: 9.4204\n",
      "Epoch 365, Loss: 9.5461\n",
      "Epoch 366, Loss: 9.3454\n",
      "Epoch 367, Loss: 9.5903\n",
      "Epoch 368, Loss: 9.6057\n",
      "Epoch 369, Loss: 9.4439\n",
      "Epoch 370, Loss: 9.5094\n",
      "Epoch 371, Loss: 9.5462\n",
      "Epoch 372, Loss: 9.5874\n",
      "Epoch 373, Loss: 9.4993\n",
      "Epoch 374, Loss: 9.5043\n",
      "Epoch 375, Loss: 9.4373\n",
      "Epoch 376, Loss: 9.6997\n",
      "Epoch 377, Loss: 9.4403\n",
      "Epoch 378, Loss: 9.4179\n",
      "Epoch 379, Loss: 9.4765\n",
      "Epoch 380, Loss: 9.5469\n",
      "Epoch 381, Loss: 9.3859\n",
      "Epoch 382, Loss: 9.4352\n",
      "Epoch 383, Loss: 9.5122\n",
      "Epoch 384, Loss: 9.3504\n",
      "Epoch 385, Loss: 9.4793\n",
      "Epoch 386, Loss: 9.3538\n",
      "Epoch 387, Loss: 9.5114\n",
      "Epoch 388, Loss: 9.4055\n",
      "Epoch 389, Loss: 9.3147\n",
      "Epoch 390, Loss: 9.4149\n",
      "Epoch 391, Loss: 9.4196\n",
      "Epoch 392, Loss: 9.4471\n",
      "Epoch 393, Loss: 9.4607\n",
      "Epoch 394, Loss: 9.5447\n",
      "Epoch 395, Loss: 9.4401\n",
      "Epoch 396, Loss: 9.4797\n",
      "Epoch 397, Loss: 9.4080\n",
      "Epoch 398, Loss: 9.4127\n",
      "Epoch 399, Loss: 9.3320\n",
      "Epoch 400, Loss: 9.4114\n",
      "Epoch 401, Loss: 9.3315\n",
      "Epoch 402, Loss: 9.2552\n",
      "Epoch 403, Loss: 9.3382\n",
      "Epoch 404, Loss: 9.3749\n",
      "Epoch 405, Loss: 9.2952\n",
      "Epoch 406, Loss: 9.4781\n",
      "Epoch 407, Loss: 9.3093\n",
      "Epoch 408, Loss: 9.3263\n",
      "Epoch 409, Loss: 9.1708\n",
      "Epoch 410, Loss: 9.2748\n",
      "Epoch 411, Loss: 9.2916\n",
      "Epoch 412, Loss: 9.3792\n",
      "Epoch 413, Loss: 9.2314\n",
      "Epoch 414, Loss: 9.2057\n",
      "Epoch 415, Loss: 9.2850\n",
      "Epoch 416, Loss: 9.3478\n",
      "Epoch 417, Loss: 9.3237\n",
      "Epoch 418, Loss: 9.3712\n",
      "Epoch 419, Loss: 9.3538\n",
      "Epoch 420, Loss: 9.1982\n",
      "Epoch 421, Loss: 9.2063\n",
      "Epoch 422, Loss: 9.2157\n",
      "Epoch 423, Loss: 9.1733\n",
      "Epoch 424, Loss: 9.1607\n",
      "Epoch 425, Loss: 9.2563\n",
      "Epoch 426, Loss: 9.1753\n",
      "Epoch 427, Loss: 9.4082\n",
      "Epoch 428, Loss: 9.2529\n",
      "Epoch 429, Loss: 9.4427\n",
      "Epoch 430, Loss: 9.2443\n",
      "Epoch 431, Loss: 9.2393\n",
      "Epoch 432, Loss: 9.0984\n",
      "Epoch 433, Loss: 9.2404\n",
      "Epoch 434, Loss: 9.3939\n",
      "Epoch 435, Loss: 9.3359\n",
      "Epoch 436, Loss: 9.2342\n",
      "Epoch 437, Loss: 9.2956\n",
      "Epoch 438, Loss: 9.2398\n",
      "Epoch 439, Loss: 9.2928\n",
      "Epoch 440, Loss: 9.0986\n",
      "Epoch 441, Loss: 9.2564\n",
      "Epoch 442, Loss: 9.1015\n",
      "Epoch 443, Loss: 9.1394\n",
      "Epoch 444, Loss: 9.3013\n",
      "Epoch 445, Loss: 9.2536\n",
      "Epoch 446, Loss: 9.1901\n",
      "Epoch 447, Loss: 9.3202\n",
      "Epoch 448, Loss: 9.2045\n",
      "Epoch 449, Loss: 9.2992\n",
      "Epoch 450, Loss: 9.2451\n",
      "Epoch 451, Loss: 9.1842\n",
      "Epoch 452, Loss: 9.2055\n",
      "Epoch 453, Loss: 9.0686\n",
      "Epoch 454, Loss: 9.2033\n",
      "Epoch 455, Loss: 9.1362\n",
      "Epoch 456, Loss: 9.2361\n",
      "Epoch 457, Loss: 9.1306\n",
      "Epoch 458, Loss: 9.2218\n",
      "Epoch 459, Loss: 9.1427\n",
      "Epoch 460, Loss: 9.2125\n",
      "Epoch 461, Loss: 9.2400\n",
      "Epoch 462, Loss: 9.1528\n",
      "Epoch 463, Loss: 9.0838\n",
      "Epoch 464, Loss: 9.1817\n",
      "Epoch 465, Loss: 9.1376\n",
      "Epoch 466, Loss: 9.1591\n",
      "Epoch 467, Loss: 9.2671\n",
      "Epoch 468, Loss: 9.1000\n",
      "Epoch 469, Loss: 9.1013\n",
      "Epoch 470, Loss: 9.1264\n",
      "Epoch 471, Loss: 9.1049\n",
      "Epoch 472, Loss: 9.2813\n",
      "Epoch 473, Loss: 9.1553\n",
      "Epoch 474, Loss: 9.2103\n",
      "Epoch 475, Loss: 9.1212\n",
      "Epoch 476, Loss: 9.2219\n",
      "Epoch 477, Loss: 9.0056\n",
      "Epoch 478, Loss: 9.1069\n",
      "Epoch 479, Loss: 9.0779\n",
      "Epoch 480, Loss: 9.1933\n",
      "Epoch 481, Loss: 9.1860\n",
      "Epoch 482, Loss: 9.2503\n",
      "Epoch 483, Loss: 9.2743\n",
      "Epoch 484, Loss: 9.2745\n",
      "Epoch 485, Loss: 9.1282\n",
      "Epoch 486, Loss: 9.1306\n",
      "Epoch 487, Loss: 9.2754\n",
      "Epoch 488, Loss: 9.0969\n",
      "Epoch 489, Loss: 9.2240\n",
      "Epoch 490, Loss: 9.1779\n",
      "Epoch 491, Loss: 9.1967\n",
      "Epoch 492, Loss: 9.1164\n",
      "Epoch 493, Loss: 9.1975\n",
      "Epoch 494, Loss: 9.2300\n",
      "Epoch 495, Loss: 9.2078\n",
      "Epoch 496, Loss: 9.2626\n",
      "Epoch 497, Loss: 9.1923\n",
      "Epoch 498, Loss: 9.1436\n",
      "Epoch 499, Loss: 9.0703\n",
      "Epoch 500, Loss: 9.0806\n"
     ]
    }
   ],
   "source": [
    "train(epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e0098e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08aa2881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated binary palindromic sequences:\n",
      "10001001011010010001\n",
      "11000001100110000011\n",
      "01011001100110011010\n",
      "11110011000011001111\n",
      "01010101100110101010\n",
      "00111101011010111100\n",
      "01101101111110110110\n",
      "00101100100100110101\n",
      "11011010100101011011\n",
      "01001111100111110010\n",
      "00010001100110001000\n",
      "00101110111101110100\n",
      "11000111011011100011\n",
      "01100111011011100110\n",
      "01011100111100011010\n",
      "00010000100100001000\n",
      "10100110000001100101\n",
      "00011111000011111000\n",
      "11001001100110010011\n",
      "11101001011010010111\n",
      "01000000000000000010\n",
      "11100010000001000111\n",
      "01100100100100100110\n",
      "11101000011000010111\n",
      "00010011000011001000\n",
      "00111110011001111100\n",
      "10011010100101011001\n",
      "01101110011001110110\n",
      "00101011100111010100\n",
      "11010101000010101011\n",
      "10001001000010110001\n",
      "10101000100100010101\n",
      "10101011011011010101\n",
      "10001010100101010001\n",
      "11101011011011010111\n",
      "01010111000011101010\n",
      "01000001100110000010\n",
      "10000010100101000001\n",
      "11010010111101001011\n",
      "11100000111100000111\n",
      "01111100000000111110\n",
      "00101110011001010100\n",
      "11001010111101010011\n",
      "10100010000001000101\n",
      "00000001100110000000\n",
      "00001011111111010000\n",
      "00111101100110111100\n",
      "00111110000001111100\n",
      "11100011000011000111\n",
      "11110000000000001111\n",
      "11001011111111010011\n",
      "01110110000001101110\n",
      "10111001100110011101\n",
      "01110110100101101110\n",
      "11100000100100000111\n",
      "01010110011001101010\n",
      "11000000100100000011\n",
      "01010011100111001010\n",
      "11000101011010100011\n",
      "01111111000011111110\n",
      "10010010111101001001\n",
      "10111010111101011101\n",
      "11001000011000010011\n",
      "00100011100111000100\n",
      "11111000100100011011\n",
      "00101101000010110100\n",
      "11001001111110010011\n",
      "11111010100101011111\n",
      "01100111011011100110\n",
      "10000001011010000001\n",
      "10100001011010000101\n",
      "00111110111101111100\n",
      "10111010011001011101\n",
      "01010010000001001010\n",
      "00111101000010111100\n",
      "00000111011011100000\n",
      "11101110000001110111\n",
      "00000001100110000000\n",
      "01010110000001101010\n",
      "11010000011000001011\n",
      "01000101000010100010\n",
      "00011111000011011000\n",
      "11110101011010101111\n",
      "01101100100100110110\n",
      "01001111100111110110\n",
      "10100001100110000101\n",
      "11011011100111011011\n",
      "01110101100110001110\n",
      "10111110100101111101\n",
      "00111110111100111100\n",
      "10010001000010001001\n",
      "10010010000001001001\n",
      "00001010100101010000\n",
      "11110010111101101111\n",
      "00000101011010100000\n",
      "11000000011000100011\n",
      "11111001111110011111\n",
      "10100010011001000101\n",
      "11100000000000000111\n",
      "00111101100110111100\n",
      "\n",
      "Total generated: 100\n",
      "Correct palindromic sequences: 89\n",
      "Accuracy: 89.00%\n"
     ]
    }
   ],
   "source": [
    "vae.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(100, 10)  # 100 samples from the latent space\n",
    "    generated = vae.decode(z).round()\n",
    "    generated_sequences = [''.join(map(str, seq.int().tolist())) for seq in generated]\n",
    "\n",
    "    correct_palindromes = [seq for seq in generated_sequences if is_palindrome(seq)]\n",
    "\n",
    "    print(\"Generated binary palindromic sequences:\")\n",
    "    for seq in generated_sequences:\n",
    "        print(seq)\n",
    "\n",
    "    print(f\"\\nTotal generated: {len(generated_sequences)}\")\n",
    "    print(f\"Correct palindromic sequences: {len(correct_palindromes)}\")\n",
    "    print(f\"Accuracy: {len(correct_palindromes) / len(generated_sequences) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d7a0f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f70f160",
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:micromamba-dem]",
   "language": "python",
   "name": "conda-env-micromamba-dem-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
