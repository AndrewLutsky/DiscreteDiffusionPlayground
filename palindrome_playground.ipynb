{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22751550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "import random\n",
    "from torch import Tensor\n",
    "import math\n",
    "from torch.nn import TransformerDecoder, TransformerDecoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a6f86bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9680fba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079c97bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_palindromic_binary_data(num_samples=1000, length=20):\n",
    "    data = []\n",
    "    for _ in range(num_samples):\n",
    "        half = np.random.randint(0, 2, size=length // 2)\n",
    "        palindrome = np.concatenate((half, half[::-1]))\n",
    "        data.append(palindrome)\n",
    "    return torch.tensor(data, dtype=torch.float32)\n",
    "def is_palindrome(seq):\n",
    "    return seq == seq[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb67773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader\n",
    "batch_size = 32\n",
    "binary_palindromes = generate_palindromic_binary_data()\n",
    "train_loader = DataLoader(TensorDataset(binary_palindromes), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fdc407",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 20):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10402476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Class, decoder only attempt\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size=2, d_model=64, nhead=2, d_hid=256, num_layers=1, dropout=0.2):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size + 1, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        decoder_layers = TransformerDecoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_decoder = TransformerDecoder(decoder_layers, num_layers)\n",
    "        self.d_model = d_model\n",
    "        self.linear = nn.Linear(d_model, vocab_size)\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) * math.sqrt(self.d_model)  \n",
    "        x = self.pos_encoder(x)\n",
    "        mask = self.generate_mask(x.size(0)) \n",
    "        x = self.transformer_decoder(x, x, tgt_mask=mask)  # Self-attention?\n",
    "        x = self.linear(x) \n",
    "        return torch.softmax(x, dim=-1) #\n",
    "    \n",
    "    def generate_mask(self, size):\n",
    "        return torch.triu(torch.full((size, size), float('-inf')), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9c656ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11d4029e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=2)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c463c34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Avg Loss: 0.3876\n",
      "Epoch 2, Avg Loss: 0.3870\n",
      "Epoch 3, Avg Loss: 0.3842\n",
      "Epoch 4, Avg Loss: 0.3856\n",
      "Epoch 5, Avg Loss: 0.3866\n",
      "Epoch 6, Avg Loss: 0.3861\n",
      "Epoch 7, Avg Loss: 0.3826\n",
      "Epoch 8, Avg Loss: 0.3836\n",
      "Epoch 9, Avg Loss: 0.3857\n",
      "Epoch 10, Avg Loss: 0.3831\n",
      "Epoch 11, Avg Loss: 0.3840\n",
      "Epoch 12, Avg Loss: 0.3835\n",
      "Epoch 13, Avg Loss: 0.3818\n",
      "Epoch 14, Avg Loss: 0.3902\n",
      "Epoch 15, Avg Loss: 0.3844\n",
      "Epoch 16, Avg Loss: 0.3831\n",
      "Epoch 17, Avg Loss: 0.3864\n",
      "Epoch 18, Avg Loss: 0.3828\n",
      "Epoch 19, Avg Loss: 0.3846\n",
      "Epoch 20, Avg Loss: 0.3846\n",
      "Epoch 21, Avg Loss: 0.3842\n",
      "Epoch 22, Avg Loss: 0.3853\n",
      "Epoch 23, Avg Loss: 0.3835\n",
      "Epoch 24, Avg Loss: 0.3814\n",
      "Epoch 25, Avg Loss: 0.3850\n",
      "Epoch 26, Avg Loss: 0.3916\n",
      "Epoch 27, Avg Loss: 0.3843\n",
      "Epoch 28, Avg Loss: 0.3851\n",
      "Epoch 29, Avg Loss: 0.3839\n",
      "Epoch 30, Avg Loss: 0.3861\n",
      "Epoch 31, Avg Loss: 0.3850\n",
      "Epoch 32, Avg Loss: 0.3853\n",
      "Epoch 33, Avg Loss: 0.3858\n",
      "Epoch 34, Avg Loss: 0.3853\n",
      "Epoch 35, Avg Loss: 0.3842\n",
      "Epoch 36, Avg Loss: 0.3840\n",
      "Epoch 37, Avg Loss: 0.3861\n",
      "Epoch 38, Avg Loss: 0.3865\n",
      "Epoch 39, Avg Loss: 0.3843\n",
      "Epoch 40, Avg Loss: 0.3857\n",
      "Epoch 41, Avg Loss: 0.3885\n",
      "Epoch 42, Avg Loss: 0.3850\n",
      "Epoch 43, Avg Loss: 0.3883\n",
      "Epoch 44, Avg Loss: 0.3857\n",
      "Epoch 45, Avg Loss: 0.3833\n",
      "Epoch 46, Avg Loss: 0.3843\n",
      "Epoch 47, Avg Loss: 0.3828\n",
      "Epoch 48, Avg Loss: 0.3847\n",
      "Epoch 49, Avg Loss: 0.3869\n",
      "Epoch 50, Avg Loss: 0.3849\n",
      "Epoch 51, Avg Loss: 0.3825\n",
      "Epoch 52, Avg Loss: 0.3881\n",
      "Epoch 53, Avg Loss: 0.3896\n",
      "Epoch 54, Avg Loss: 0.3831\n",
      "Epoch 55, Avg Loss: 0.3839\n",
      "Epoch 56, Avg Loss: 0.3843\n",
      "Epoch 57, Avg Loss: 0.3826\n",
      "Epoch 58, Avg Loss: 0.3854\n",
      "Epoch 59, Avg Loss: 0.3870\n",
      "Epoch 60, Avg Loss: 0.3881\n",
      "Epoch 61, Avg Loss: 0.3849\n",
      "Epoch 62, Avg Loss: 0.3844\n",
      "Epoch 63, Avg Loss: 0.3856\n",
      "Epoch 64, Avg Loss: 0.3880\n",
      "Epoch 65, Avg Loss: 0.3856\n",
      "Epoch 66, Avg Loss: 0.3850\n",
      "Epoch 67, Avg Loss: 0.3818\n",
      "Epoch 68, Avg Loss: 0.3851\n",
      "Epoch 69, Avg Loss: 0.3851\n",
      "Epoch 70, Avg Loss: 0.3873\n",
      "Epoch 71, Avg Loss: 0.3869\n",
      "Epoch 72, Avg Loss: 0.3840\n",
      "Epoch 73, Avg Loss: 0.3868\n",
      "Epoch 74, Avg Loss: 0.3866\n",
      "Epoch 75, Avg Loss: 0.3859\n",
      "Epoch 76, Avg Loss: 0.3828\n",
      "Epoch 77, Avg Loss: 0.3846\n",
      "Epoch 78, Avg Loss: 0.3854\n",
      "Epoch 79, Avg Loss: 0.3861\n",
      "Epoch 80, Avg Loss: 0.3854\n",
      "Epoch 81, Avg Loss: 0.3825\n",
      "Epoch 82, Avg Loss: 0.3845\n",
      "Epoch 83, Avg Loss: 0.3803\n",
      "Epoch 84, Avg Loss: 0.3841\n",
      "Epoch 85, Avg Loss: 0.3860\n",
      "Epoch 86, Avg Loss: 0.3865\n",
      "Epoch 87, Avg Loss: 0.3883\n",
      "Epoch 88, Avg Loss: 0.3897\n",
      "Epoch 89, Avg Loss: 0.3822\n",
      "Epoch 90, Avg Loss: 0.3857\n",
      "Epoch 91, Avg Loss: 0.3841\n",
      "Epoch 92, Avg Loss: 0.3851\n",
      "Epoch 93, Avg Loss: 0.3850\n",
      "Epoch 94, Avg Loss: 0.3857\n",
      "Epoch 95, Avg Loss: 0.3830\n",
      "Epoch 96, Avg Loss: 0.3886\n",
      "Epoch 97, Avg Loss: 0.3835\n",
      "Epoch 98, Avg Loss: 0.3834\n",
      "Epoch 99, Avg Loss: 0.3846\n",
      "Epoch 100, Avg Loss: 0.3870\n",
      "Epoch 101, Avg Loss: 0.3866\n",
      "Epoch 102, Avg Loss: 0.3840\n",
      "Epoch 103, Avg Loss: 0.3825\n",
      "Epoch 104, Avg Loss: 0.3825\n",
      "Epoch 105, Avg Loss: 0.3845\n",
      "Epoch 106, Avg Loss: 0.3862\n",
      "Epoch 107, Avg Loss: 0.3904\n",
      "Epoch 108, Avg Loss: 0.3852\n",
      "Epoch 109, Avg Loss: 0.3862\n",
      "Epoch 110, Avg Loss: 0.3888\n",
      "Epoch 111, Avg Loss: 0.3861\n",
      "Epoch 112, Avg Loss: 0.3829\n",
      "Epoch 113, Avg Loss: 0.3850\n",
      "Epoch 114, Avg Loss: 0.3859\n",
      "Epoch 115, Avg Loss: 0.3858\n",
      "Epoch 116, Avg Loss: 0.3812\n",
      "Epoch 117, Avg Loss: 0.3835\n",
      "Epoch 118, Avg Loss: 0.3829\n",
      "Epoch 119, Avg Loss: 0.3847\n",
      "Epoch 120, Avg Loss: 0.3854\n",
      "Epoch 121, Avg Loss: 0.3892\n",
      "Epoch 122, Avg Loss: 0.3923\n",
      "Epoch 123, Avg Loss: 0.3854\n",
      "Epoch 124, Avg Loss: 0.3854\n",
      "Epoch 125, Avg Loss: 0.3842\n",
      "Epoch 126, Avg Loss: 0.3840\n",
      "Epoch 127, Avg Loss: 0.3895\n",
      "Epoch 128, Avg Loss: 0.3852\n",
      "Epoch 129, Avg Loss: 0.3850\n",
      "Epoch 130, Avg Loss: 0.3852\n",
      "Epoch 131, Avg Loss: 0.3855\n",
      "Epoch 132, Avg Loss: 0.3846\n",
      "Epoch 133, Avg Loss: 0.3830\n",
      "Epoch 134, Avg Loss: 0.3890\n",
      "Epoch 135, Avg Loss: 0.3864\n",
      "Epoch 136, Avg Loss: 0.3879\n",
      "Epoch 137, Avg Loss: 0.3826\n",
      "Epoch 138, Avg Loss: 0.3858\n",
      "Epoch 139, Avg Loss: 0.3831\n",
      "Epoch 140, Avg Loss: 0.3833\n",
      "Epoch 141, Avg Loss: 0.3872\n",
      "Epoch 142, Avg Loss: 0.3849\n",
      "Epoch 143, Avg Loss: 0.3849\n",
      "Epoch 144, Avg Loss: 0.3849\n",
      "Epoch 145, Avg Loss: 0.3846\n",
      "Epoch 146, Avg Loss: 0.3840\n",
      "Epoch 147, Avg Loss: 0.3835\n",
      "Epoch 148, Avg Loss: 0.3864\n",
      "Epoch 149, Avg Loss: 0.3854\n",
      "Epoch 150, Avg Loss: 0.3834\n",
      "Epoch 151, Avg Loss: 0.3832\n",
      "Epoch 152, Avg Loss: 0.3875\n",
      "Epoch 153, Avg Loss: 0.3854\n",
      "Epoch 154, Avg Loss: 0.3883\n",
      "Epoch 155, Avg Loss: 0.3861\n",
      "Epoch 156, Avg Loss: 0.3841\n",
      "Epoch 157, Avg Loss: 0.3861\n",
      "Epoch 158, Avg Loss: 0.3859\n",
      "Epoch 159, Avg Loss: 0.3895\n",
      "Epoch 160, Avg Loss: 0.3856\n",
      "Epoch 161, Avg Loss: 0.3866\n",
      "Epoch 162, Avg Loss: 0.3834\n",
      "Epoch 163, Avg Loss: 0.3874\n",
      "Epoch 164, Avg Loss: 0.3858\n",
      "Epoch 165, Avg Loss: 0.3847\n",
      "Epoch 166, Avg Loss: 0.3834\n",
      "Epoch 167, Avg Loss: 0.3855\n",
      "Epoch 168, Avg Loss: 0.3840\n",
      "Epoch 169, Avg Loss: 0.3832\n",
      "Epoch 170, Avg Loss: 0.3878\n",
      "Epoch 171, Avg Loss: 0.3863\n",
      "Epoch 172, Avg Loss: 0.3851\n",
      "Epoch 173, Avg Loss: 0.3977\n",
      "Epoch 174, Avg Loss: 0.3928\n",
      "Epoch 175, Avg Loss: 0.3861\n",
      "Epoch 176, Avg Loss: 0.3831\n",
      "Epoch 177, Avg Loss: 0.3852\n",
      "Epoch 178, Avg Loss: 0.3842\n",
      "Epoch 179, Avg Loss: 0.3846\n",
      "Epoch 180, Avg Loss: 0.3835\n",
      "Epoch 181, Avg Loss: 0.3837\n",
      "Epoch 182, Avg Loss: 0.3867\n",
      "Epoch 183, Avg Loss: 0.3870\n",
      "Epoch 184, Avg Loss: 0.3870\n",
      "Epoch 185, Avg Loss: 0.3854\n",
      "Epoch 186, Avg Loss: 0.3893\n",
      "Epoch 187, Avg Loss: 0.3868\n",
      "Epoch 188, Avg Loss: 0.3874\n",
      "Epoch 189, Avg Loss: 0.3859\n",
      "Epoch 190, Avg Loss: 0.3844\n",
      "Epoch 191, Avg Loss: 0.3873\n",
      "Epoch 192, Avg Loss: 0.3844\n",
      "Epoch 193, Avg Loss: 0.3840\n",
      "Epoch 194, Avg Loss: 0.3850\n",
      "Epoch 195, Avg Loss: 0.3844\n",
      "Epoch 196, Avg Loss: 0.3851\n",
      "Epoch 197, Avg Loss: 0.3846\n",
      "Epoch 198, Avg Loss: 0.3868\n",
      "Epoch 199, Avg Loss: 0.3844\n",
      "Epoch 200, Avg Loss: 0.3841\n",
      "Epoch 201, Avg Loss: 0.3876\n",
      "Epoch 202, Avg Loss: 0.3871\n",
      "Epoch 203, Avg Loss: 0.3838\n",
      "Epoch 204, Avg Loss: 0.3812\n",
      "Epoch 205, Avg Loss: 0.3843\n",
      "Epoch 206, Avg Loss: 0.3918\n",
      "Epoch 207, Avg Loss: 0.3845\n",
      "Epoch 208, Avg Loss: 0.3895\n",
      "Epoch 209, Avg Loss: 0.3900\n",
      "Epoch 210, Avg Loss: 0.3894\n",
      "Epoch 211, Avg Loss: 0.3928\n",
      "Epoch 212, Avg Loss: 0.3867\n",
      "Epoch 213, Avg Loss: 0.3878\n",
      "Epoch 214, Avg Loss: 0.3883\n",
      "Epoch 215, Avg Loss: 0.3851\n",
      "Epoch 216, Avg Loss: 0.3877\n",
      "Epoch 217, Avg Loss: 0.3830\n",
      "Epoch 218, Avg Loss: 0.3868\n",
      "Epoch 219, Avg Loss: 0.3840\n",
      "Epoch 220, Avg Loss: 0.3849\n",
      "Epoch 221, Avg Loss: 0.3834\n",
      "Epoch 222, Avg Loss: 0.3899\n",
      "Epoch 223, Avg Loss: 0.3855\n",
      "Epoch 224, Avg Loss: 0.3855\n",
      "Epoch 225, Avg Loss: 0.3880\n",
      "Epoch 226, Avg Loss: 0.3841\n",
      "Epoch 227, Avg Loss: 0.3870\n",
      "Epoch 228, Avg Loss: 0.3832\n",
      "Epoch 229, Avg Loss: 0.3833\n",
      "Epoch 230, Avg Loss: 0.3884\n",
      "Epoch 231, Avg Loss: 0.3857\n",
      "Epoch 232, Avg Loss: 0.3858\n",
      "Epoch 233, Avg Loss: 0.3897\n",
      "Epoch 234, Avg Loss: 0.3851\n",
      "Epoch 235, Avg Loss: 0.3864\n",
      "Epoch 236, Avg Loss: 0.3893\n",
      "Epoch 237, Avg Loss: 0.3918\n",
      "Epoch 238, Avg Loss: 0.3849\n",
      "Epoch 239, Avg Loss: 0.3920\n",
      "Epoch 240, Avg Loss: 0.3860\n",
      "Epoch 241, Avg Loss: 0.3831\n",
      "Epoch 242, Avg Loss: 0.3828\n",
      "Epoch 243, Avg Loss: 0.3856\n",
      "Epoch 244, Avg Loss: 0.3855\n",
      "Epoch 245, Avg Loss: 0.3893\n",
      "Epoch 246, Avg Loss: 0.3857\n",
      "Epoch 247, Avg Loss: 0.3868\n",
      "Epoch 248, Avg Loss: 0.3862\n",
      "Epoch 249, Avg Loss: 0.3850\n",
      "Epoch 250, Avg Loss: 0.3880\n",
      "Epoch 251, Avg Loss: 0.3863\n",
      "Epoch 252, Avg Loss: 0.3838\n",
      "Epoch 253, Avg Loss: 0.3848\n",
      "Epoch 254, Avg Loss: 0.3844\n",
      "Epoch 255, Avg Loss: 0.3851\n",
      "Epoch 256, Avg Loss: 0.3848\n",
      "Epoch 257, Avg Loss: 0.3858\n",
      "Epoch 258, Avg Loss: 0.3834\n",
      "Epoch 259, Avg Loss: 0.3868\n",
      "Epoch 260, Avg Loss: 0.3867\n",
      "Epoch 261, Avg Loss: 0.3840\n",
      "Epoch 262, Avg Loss: 0.3911\n",
      "Epoch 263, Avg Loss: 0.3857\n",
      "Epoch 264, Avg Loss: 0.3868\n",
      "Epoch 265, Avg Loss: 0.3858\n",
      "Epoch 266, Avg Loss: 0.3867\n",
      "Epoch 267, Avg Loss: 0.3860\n",
      "Epoch 268, Avg Loss: 0.3893\n",
      "Epoch 269, Avg Loss: 0.3883\n",
      "Epoch 270, Avg Loss: 0.3859\n",
      "Epoch 271, Avg Loss: 0.3847\n",
      "Epoch 272, Avg Loss: 0.3908\n",
      "Epoch 273, Avg Loss: 0.3870\n",
      "Epoch 274, Avg Loss: 0.3885\n",
      "Epoch 275, Avg Loss: 0.3842\n",
      "Epoch 276, Avg Loss: 0.3832\n",
      "Epoch 277, Avg Loss: 0.3853\n",
      "Epoch 278, Avg Loss: 0.3847\n",
      "Epoch 279, Avg Loss: 0.3847\n",
      "Epoch 280, Avg Loss: 0.3857\n",
      "Epoch 281, Avg Loss: 0.3845\n",
      "Epoch 282, Avg Loss: 0.3845\n",
      "Epoch 283, Avg Loss: 0.3866\n",
      "Epoch 284, Avg Loss: 0.3884\n",
      "Epoch 285, Avg Loss: 0.3866\n",
      "Epoch 286, Avg Loss: 0.3875\n",
      "Epoch 287, Avg Loss: 0.3883\n",
      "Epoch 288, Avg Loss: 0.3902\n",
      "Epoch 289, Avg Loss: 0.3947\n",
      "Epoch 290, Avg Loss: 0.3933\n",
      "Epoch 291, Avg Loss: 0.3853\n",
      "Epoch 292, Avg Loss: 0.3846\n",
      "Epoch 293, Avg Loss: 0.3866\n",
      "Epoch 294, Avg Loss: 0.3859\n",
      "Epoch 295, Avg Loss: 0.3832\n",
      "Epoch 296, Avg Loss: 0.3842\n",
      "Epoch 297, Avg Loss: 0.3854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 298, Avg Loss: 0.3876\n",
      "Epoch 299, Avg Loss: 0.3846\n",
      "Epoch 300, Avg Loss: 0.3851\n",
      "Epoch 301, Avg Loss: 0.3859\n",
      "Epoch 302, Avg Loss: 0.3876\n",
      "Epoch 303, Avg Loss: 0.3912\n",
      "Epoch 304, Avg Loss: 0.3855\n",
      "Epoch 305, Avg Loss: 0.3837\n",
      "Epoch 306, Avg Loss: 0.3866\n",
      "Epoch 307, Avg Loss: 0.3993\n",
      "Epoch 308, Avg Loss: 0.3870\n",
      "Epoch 309, Avg Loss: 0.3855\n",
      "Epoch 310, Avg Loss: 0.3823\n",
      "Epoch 311, Avg Loss: 0.3877\n",
      "Epoch 312, Avg Loss: 0.3837\n",
      "Epoch 313, Avg Loss: 0.3913\n",
      "Epoch 314, Avg Loss: 0.3920\n",
      "Epoch 315, Avg Loss: 0.3888\n",
      "Epoch 316, Avg Loss: 0.3856\n",
      "Epoch 317, Avg Loss: 0.3922\n",
      "Epoch 318, Avg Loss: 0.3877\n",
      "Epoch 319, Avg Loss: 0.3910\n",
      "Epoch 320, Avg Loss: 0.3901\n",
      "Epoch 321, Avg Loss: 0.3865\n",
      "Epoch 322, Avg Loss: 0.3853\n",
      "Epoch 323, Avg Loss: 0.3850\n",
      "Epoch 324, Avg Loss: 0.3832\n",
      "Epoch 325, Avg Loss: 0.3816\n",
      "Epoch 326, Avg Loss: 0.3856\n",
      "Epoch 327, Avg Loss: 0.3841\n",
      "Epoch 328, Avg Loss: 0.3831\n",
      "Epoch 329, Avg Loss: 0.3844\n",
      "Epoch 330, Avg Loss: 0.3836\n",
      "Epoch 331, Avg Loss: 0.3906\n",
      "Epoch 332, Avg Loss: 0.3869\n",
      "Epoch 333, Avg Loss: 0.3873\n",
      "Epoch 334, Avg Loss: 0.3865\n",
      "Epoch 335, Avg Loss: 0.3841\n",
      "Epoch 336, Avg Loss: 0.3849\n",
      "Epoch 337, Avg Loss: 0.3875\n",
      "Epoch 338, Avg Loss: 0.3884\n",
      "Epoch 339, Avg Loss: 0.3871\n",
      "Epoch 340, Avg Loss: 0.3850\n",
      "Epoch 341, Avg Loss: 0.3852\n",
      "Epoch 342, Avg Loss: 0.3899\n",
      "Epoch 343, Avg Loss: 0.3870\n",
      "Epoch 344, Avg Loss: 0.3908\n",
      "Epoch 345, Avg Loss: 0.3853\n",
      "Epoch 346, Avg Loss: 0.3848\n",
      "Epoch 347, Avg Loss: 0.3936\n",
      "Epoch 348, Avg Loss: 0.3906\n",
      "Epoch 349, Avg Loss: 0.3882\n",
      "Epoch 350, Avg Loss: 0.3849\n",
      "Epoch 351, Avg Loss: 0.3858\n",
      "Epoch 352, Avg Loss: 0.3915\n",
      "Epoch 353, Avg Loss: 0.3823\n",
      "Epoch 354, Avg Loss: 0.3835\n",
      "Epoch 355, Avg Loss: 0.3863\n",
      "Epoch 356, Avg Loss: 0.3882\n",
      "Epoch 357, Avg Loss: 0.3867\n",
      "Epoch 358, Avg Loss: 0.3868\n",
      "Epoch 359, Avg Loss: 0.3913\n",
      "Epoch 360, Avg Loss: 0.3879\n",
      "Epoch 361, Avg Loss: 0.3845\n",
      "Epoch 362, Avg Loss: 0.3873\n",
      "Epoch 363, Avg Loss: 0.3896\n",
      "Epoch 364, Avg Loss: 0.3867\n",
      "Epoch 365, Avg Loss: 0.3863\n",
      "Epoch 366, Avg Loss: 0.3865\n",
      "Epoch 367, Avg Loss: 0.3890\n",
      "Epoch 368, Avg Loss: 0.3911\n",
      "Epoch 369, Avg Loss: 0.3917\n",
      "Epoch 370, Avg Loss: 0.3862\n",
      "Epoch 371, Avg Loss: 0.3868\n",
      "Epoch 372, Avg Loss: 0.3856\n",
      "Epoch 373, Avg Loss: 0.3851\n",
      "Epoch 374, Avg Loss: 0.3837\n",
      "Epoch 375, Avg Loss: 0.3834\n",
      "Epoch 376, Avg Loss: 0.3895\n",
      "Epoch 377, Avg Loss: 0.3868\n",
      "Epoch 378, Avg Loss: 0.3855\n",
      "Epoch 379, Avg Loss: 0.3876\n",
      "Epoch 380, Avg Loss: 0.3870\n",
      "Epoch 381, Avg Loss: 0.3856\n",
      "Epoch 382, Avg Loss: 0.3862\n",
      "Epoch 383, Avg Loss: 0.3856\n",
      "Epoch 384, Avg Loss: 0.3850\n",
      "Epoch 385, Avg Loss: 0.3834\n",
      "Epoch 386, Avg Loss: 0.3879\n",
      "Epoch 387, Avg Loss: 0.3861\n",
      "Epoch 388, Avg Loss: 0.3879\n",
      "Epoch 389, Avg Loss: 0.3902\n",
      "Epoch 390, Avg Loss: 0.3888\n",
      "Epoch 391, Avg Loss: 0.3840\n",
      "Epoch 392, Avg Loss: 0.3827\n",
      "Epoch 393, Avg Loss: 0.3843\n",
      "Epoch 394, Avg Loss: 0.3904\n",
      "Epoch 395, Avg Loss: 0.3829\n",
      "Epoch 396, Avg Loss: 0.3845\n",
      "Epoch 397, Avg Loss: 0.3820\n",
      "Epoch 398, Avg Loss: 0.3845\n",
      "Epoch 399, Avg Loss: 0.3836\n",
      "Epoch 400, Avg Loss: 0.3866\n",
      "Epoch 401, Avg Loss: 0.3871\n",
      "Epoch 402, Avg Loss: 0.3878\n",
      "Epoch 403, Avg Loss: 0.3868\n",
      "Epoch 404, Avg Loss: 0.3894\n",
      "Epoch 405, Avg Loss: 0.3845\n",
      "Epoch 406, Avg Loss: 0.3910\n",
      "Epoch 407, Avg Loss: 0.3898\n",
      "Epoch 408, Avg Loss: 0.3870\n",
      "Epoch 409, Avg Loss: 0.3895\n",
      "Epoch 410, Avg Loss: 0.3859\n",
      "Epoch 411, Avg Loss: 0.3875\n",
      "Epoch 412, Avg Loss: 0.3823\n",
      "Epoch 413, Avg Loss: 0.3906\n",
      "Epoch 414, Avg Loss: 0.3843\n",
      "Epoch 415, Avg Loss: 0.3868\n",
      "Epoch 416, Avg Loss: 0.3986\n",
      "Epoch 417, Avg Loss: 0.3937\n",
      "Epoch 418, Avg Loss: 0.3918\n",
      "Epoch 419, Avg Loss: 0.3859\n",
      "Epoch 420, Avg Loss: 0.3868\n",
      "Epoch 421, Avg Loss: 0.3920\n",
      "Epoch 422, Avg Loss: 0.3859\n",
      "Epoch 423, Avg Loss: 0.3864\n",
      "Epoch 424, Avg Loss: 0.3863\n",
      "Epoch 425, Avg Loss: 0.3850\n",
      "Epoch 426, Avg Loss: 0.3858\n",
      "Epoch 427, Avg Loss: 0.3869\n",
      "Epoch 428, Avg Loss: 0.3869\n",
      "Epoch 429, Avg Loss: 0.3919\n",
      "Epoch 430, Avg Loss: 0.3853\n",
      "Epoch 431, Avg Loss: 0.3833\n",
      "Epoch 432, Avg Loss: 0.3874\n",
      "Epoch 433, Avg Loss: 0.3812\n",
      "Epoch 434, Avg Loss: 0.3880\n",
      "Epoch 435, Avg Loss: 0.3856\n",
      "Epoch 436, Avg Loss: 0.3818\n",
      "Epoch 437, Avg Loss: 0.3867\n",
      "Epoch 438, Avg Loss: 0.4042\n",
      "Epoch 439, Avg Loss: 0.3888\n",
      "Epoch 440, Avg Loss: 0.3849\n",
      "Epoch 441, Avg Loss: 0.3878\n",
      "Epoch 442, Avg Loss: 0.3874\n",
      "Epoch 443, Avg Loss: 0.3875\n",
      "Epoch 444, Avg Loss: 0.3860\n",
      "Epoch 445, Avg Loss: 0.3838\n",
      "Epoch 446, Avg Loss: 0.3880\n",
      "Epoch 447, Avg Loss: 0.4087\n",
      "Epoch 448, Avg Loss: 0.3903\n",
      "Epoch 449, Avg Loss: 0.3863\n",
      "Epoch 450, Avg Loss: 0.3867\n",
      "Epoch 451, Avg Loss: 0.3845\n",
      "Epoch 452, Avg Loss: 0.3896\n",
      "Epoch 453, Avg Loss: 0.3880\n",
      "Epoch 454, Avg Loss: 0.3909\n",
      "Epoch 455, Avg Loss: 0.3856\n",
      "Epoch 456, Avg Loss: 0.3859\n",
      "Epoch 457, Avg Loss: 0.3844\n",
      "Epoch 458, Avg Loss: 0.3870\n",
      "Epoch 459, Avg Loss: 0.3861\n",
      "Epoch 460, Avg Loss: 0.3859\n",
      "Epoch 461, Avg Loss: 0.3927\n",
      "Epoch 462, Avg Loss: 0.3881\n",
      "Epoch 463, Avg Loss: 0.3861\n",
      "Epoch 464, Avg Loss: 0.3860\n",
      "Epoch 465, Avg Loss: 0.3855\n",
      "Epoch 466, Avg Loss: 0.3865\n",
      "Epoch 467, Avg Loss: 0.3837\n",
      "Epoch 468, Avg Loss: 0.3832\n",
      "Epoch 469, Avg Loss: 0.3880\n",
      "Epoch 470, Avg Loss: 0.3923\n",
      "Epoch 471, Avg Loss: 0.3904\n",
      "Epoch 472, Avg Loss: 0.3899\n",
      "Epoch 473, Avg Loss: 0.3880\n",
      "Epoch 474, Avg Loss: 0.3895\n",
      "Epoch 475, Avg Loss: 0.3819\n",
      "Epoch 476, Avg Loss: 0.3825\n",
      "Epoch 477, Avg Loss: 0.3854\n",
      "Epoch 478, Avg Loss: 0.3913\n",
      "Epoch 479, Avg Loss: 0.3889\n",
      "Epoch 480, Avg Loss: 0.3843\n",
      "Epoch 481, Avg Loss: 0.3836\n",
      "Epoch 482, Avg Loss: 0.3883\n",
      "Epoch 483, Avg Loss: 0.3860\n",
      "Epoch 484, Avg Loss: 0.3917\n",
      "Epoch 485, Avg Loss: 0.3889\n",
      "Epoch 486, Avg Loss: 0.3854\n",
      "Epoch 487, Avg Loss: 0.3929\n",
      "Epoch 488, Avg Loss: 0.3889\n",
      "Epoch 489, Avg Loss: 0.3854\n",
      "Epoch 490, Avg Loss: 0.3866\n",
      "Epoch 491, Avg Loss: 0.3850\n",
      "Epoch 492, Avg Loss: 0.3845\n",
      "Epoch 493, Avg Loss: 0.3910\n",
      "Epoch 494, Avg Loss: 0.3943\n",
      "Epoch 495, Avg Loss: 0.3894\n",
      "Epoch 496, Avg Loss: 0.3894\n",
      "Epoch 497, Avg Loss: 0.3880\n",
      "Epoch 498, Avg Loss: 0.3898\n",
      "Epoch 499, Avg Loss: 0.3868\n",
      "Epoch 500, Avg Loss: 0.3876\n",
      "Epoch 501, Avg Loss: 0.3856\n",
      "Epoch 502, Avg Loss: 0.3839\n",
      "Epoch 503, Avg Loss: 0.3841\n",
      "Epoch 504, Avg Loss: 0.3833\n",
      "Epoch 505, Avg Loss: 0.3875\n",
      "Epoch 506, Avg Loss: 0.3829\n",
      "Epoch 507, Avg Loss: 0.3863\n",
      "Epoch 508, Avg Loss: 0.3975\n",
      "Epoch 509, Avg Loss: 0.3891\n",
      "Epoch 510, Avg Loss: 0.3859\n",
      "Epoch 511, Avg Loss: 0.3855\n",
      "Epoch 512, Avg Loss: 0.3954\n",
      "Epoch 513, Avg Loss: 0.3925\n",
      "Epoch 514, Avg Loss: 0.3911\n",
      "Epoch 515, Avg Loss: 0.3894\n",
      "Epoch 516, Avg Loss: 0.3844\n",
      "Epoch 517, Avg Loss: 0.3920\n",
      "Epoch 518, Avg Loss: 0.3872\n",
      "Epoch 519, Avg Loss: 0.3855\n",
      "Epoch 520, Avg Loss: 0.3851\n",
      "Epoch 521, Avg Loss: 0.3879\n",
      "Epoch 522, Avg Loss: 0.3870\n",
      "Epoch 523, Avg Loss: 0.3956\n",
      "Epoch 524, Avg Loss: 0.3865\n",
      "Epoch 525, Avg Loss: 0.3908\n",
      "Epoch 526, Avg Loss: 0.3868\n",
      "Epoch 527, Avg Loss: 0.3859\n",
      "Epoch 528, Avg Loss: 0.3880\n",
      "Epoch 529, Avg Loss: 0.3921\n",
      "Epoch 530, Avg Loss: 0.3915\n",
      "Epoch 531, Avg Loss: 0.3852\n",
      "Epoch 532, Avg Loss: 0.3863\n",
      "Epoch 533, Avg Loss: 0.3829\n",
      "Epoch 534, Avg Loss: 0.3839\n",
      "Epoch 535, Avg Loss: 0.3857\n",
      "Epoch 536, Avg Loss: 0.3866\n",
      "Epoch 537, Avg Loss: 0.3930\n",
      "Epoch 538, Avg Loss: 0.3870\n",
      "Epoch 539, Avg Loss: 0.3818\n",
      "Epoch 540, Avg Loss: 0.3857\n",
      "Epoch 541, Avg Loss: 0.3903\n",
      "Epoch 542, Avg Loss: 0.3829\n",
      "Epoch 543, Avg Loss: 0.3845\n",
      "Epoch 544, Avg Loss: 0.3863\n",
      "Epoch 545, Avg Loss: 0.3826\n",
      "Epoch 546, Avg Loss: 0.3848\n",
      "Epoch 547, Avg Loss: 0.3841\n",
      "Epoch 548, Avg Loss: 0.3918\n",
      "Epoch 549, Avg Loss: 0.3911\n",
      "Epoch 550, Avg Loss: 0.3901\n",
      "Epoch 551, Avg Loss: 0.3829\n",
      "Epoch 552, Avg Loss: 0.3883\n",
      "Epoch 553, Avg Loss: 0.3981\n",
      "Epoch 554, Avg Loss: 0.3935\n",
      "Epoch 555, Avg Loss: 0.3913\n",
      "Epoch 556, Avg Loss: 0.3902\n",
      "Epoch 557, Avg Loss: 0.3886\n",
      "Epoch 558, Avg Loss: 0.3847\n",
      "Epoch 559, Avg Loss: 0.3844\n",
      "Epoch 560, Avg Loss: 0.3812\n",
      "Epoch 561, Avg Loss: 0.3873\n",
      "Epoch 562, Avg Loss: 0.3920\n",
      "Epoch 563, Avg Loss: 0.3852\n",
      "Epoch 564, Avg Loss: 0.3861\n",
      "Epoch 565, Avg Loss: 0.3842\n",
      "Epoch 566, Avg Loss: 0.3839\n",
      "Epoch 567, Avg Loss: 0.3855\n",
      "Epoch 568, Avg Loss: 0.3874\n",
      "Epoch 569, Avg Loss: 0.3848\n",
      "Epoch 570, Avg Loss: 0.3820\n",
      "Epoch 571, Avg Loss: 0.3828\n",
      "Epoch 572, Avg Loss: 0.3852\n",
      "Epoch 573, Avg Loss: 0.3880\n",
      "Epoch 574, Avg Loss: 0.3846\n",
      "Epoch 575, Avg Loss: 0.3849\n",
      "Epoch 576, Avg Loss: 0.3828\n",
      "Epoch 577, Avg Loss: 0.3795\n",
      "Epoch 578, Avg Loss: 0.3871\n",
      "Epoch 579, Avg Loss: 0.3989\n",
      "Epoch 580, Avg Loss: 0.3938\n",
      "Epoch 581, Avg Loss: 0.3844\n",
      "Epoch 582, Avg Loss: 0.3862\n",
      "Epoch 583, Avg Loss: 0.3850\n",
      "Epoch 584, Avg Loss: 0.3858\n",
      "Epoch 585, Avg Loss: 0.3934\n",
      "Epoch 586, Avg Loss: 0.3905\n",
      "Epoch 587, Avg Loss: 0.3857\n",
      "Epoch 588, Avg Loss: 0.3892\n",
      "Epoch 589, Avg Loss: 0.3869\n",
      "Epoch 590, Avg Loss: 0.3922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 591, Avg Loss: 0.3905\n",
      "Epoch 592, Avg Loss: 0.3855\n",
      "Epoch 593, Avg Loss: 0.3911\n",
      "Epoch 594, Avg Loss: 0.3876\n",
      "Epoch 595, Avg Loss: 0.3886\n",
      "Epoch 596, Avg Loss: 0.3859\n",
      "Epoch 597, Avg Loss: 0.3854\n",
      "Epoch 598, Avg Loss: 0.3929\n",
      "Epoch 599, Avg Loss: 0.3872\n",
      "Epoch 600, Avg Loss: 0.3829\n",
      "Epoch 601, Avg Loss: 0.3863\n",
      "Epoch 602, Avg Loss: 0.3829\n",
      "Epoch 603, Avg Loss: 0.3878\n",
      "Epoch 604, Avg Loss: 0.3851\n",
      "Epoch 605, Avg Loss: 0.3898\n",
      "Epoch 606, Avg Loss: 0.3848\n",
      "Epoch 607, Avg Loss: 0.3861\n",
      "Epoch 608, Avg Loss: 0.3857\n",
      "Epoch 609, Avg Loss: 0.3841\n",
      "Epoch 610, Avg Loss: 0.3900\n",
      "Epoch 611, Avg Loss: 0.3890\n",
      "Epoch 612, Avg Loss: 0.3989\n",
      "Epoch 613, Avg Loss: 0.3906\n",
      "Epoch 614, Avg Loss: 0.3855\n",
      "Epoch 615, Avg Loss: 0.3824\n",
      "Epoch 616, Avg Loss: 0.3857\n",
      "Epoch 617, Avg Loss: 0.3850\n",
      "Epoch 618, Avg Loss: 0.3892\n",
      "Epoch 619, Avg Loss: 0.3857\n",
      "Epoch 620, Avg Loss: 0.3853\n",
      "Epoch 621, Avg Loss: 0.3858\n",
      "Epoch 622, Avg Loss: 0.3844\n",
      "Epoch 623, Avg Loss: 0.3889\n",
      "Epoch 624, Avg Loss: 0.3901\n",
      "Epoch 625, Avg Loss: 0.3855\n",
      "Epoch 626, Avg Loss: 0.3850\n",
      "Epoch 627, Avg Loss: 0.3832\n",
      "Epoch 628, Avg Loss: 0.3855\n",
      "Epoch 629, Avg Loss: 0.3919\n",
      "Epoch 630, Avg Loss: 0.3893\n",
      "Epoch 631, Avg Loss: 0.3891\n",
      "Epoch 632, Avg Loss: 0.3846\n",
      "Epoch 633, Avg Loss: 0.3873\n",
      "Epoch 634, Avg Loss: 0.3802\n",
      "Epoch 635, Avg Loss: 0.3850\n",
      "Epoch 636, Avg Loss: 0.3837\n",
      "Epoch 637, Avg Loss: 0.3858\n",
      "Epoch 638, Avg Loss: 0.3835\n",
      "Epoch 639, Avg Loss: 0.3881\n",
      "Epoch 640, Avg Loss: 0.3910\n",
      "Epoch 641, Avg Loss: 0.3894\n",
      "Epoch 642, Avg Loss: 0.3859\n",
      "Epoch 643, Avg Loss: 0.3830\n",
      "Epoch 644, Avg Loss: 0.3887\n",
      "Epoch 645, Avg Loss: 0.3899\n",
      "Epoch 646, Avg Loss: 0.3844\n",
      "Epoch 647, Avg Loss: 0.3840\n",
      "Epoch 648, Avg Loss: 0.3794\n",
      "Epoch 649, Avg Loss: 0.3873\n",
      "Epoch 650, Avg Loss: 0.3890\n",
      "Epoch 651, Avg Loss: 0.3820\n",
      "Epoch 652, Avg Loss: 0.3856\n",
      "Epoch 653, Avg Loss: 0.3832\n",
      "Epoch 654, Avg Loss: 0.3852\n",
      "Epoch 655, Avg Loss: 0.3815\n",
      "Epoch 656, Avg Loss: 0.3850\n",
      "Epoch 657, Avg Loss: 0.3868\n",
      "Epoch 658, Avg Loss: 0.3856\n",
      "Epoch 659, Avg Loss: 0.3840\n",
      "Epoch 660, Avg Loss: 0.3834\n",
      "Epoch 661, Avg Loss: 0.3892\n",
      "Epoch 662, Avg Loss: 0.3849\n",
      "Epoch 663, Avg Loss: 0.3840\n",
      "Epoch 664, Avg Loss: 0.3882\n",
      "Epoch 665, Avg Loss: 0.3863\n",
      "Epoch 666, Avg Loss: 0.3823\n",
      "Epoch 667, Avg Loss: 0.3840\n",
      "Epoch 668, Avg Loss: 0.3916\n",
      "Epoch 669, Avg Loss: 0.3868\n",
      "Epoch 670, Avg Loss: 0.3846\n",
      "Epoch 671, Avg Loss: 0.3954\n",
      "Epoch 672, Avg Loss: 0.3908\n",
      "Epoch 673, Avg Loss: 0.3841\n",
      "Epoch 674, Avg Loss: 0.3894\n",
      "Epoch 675, Avg Loss: 0.3818\n",
      "Epoch 676, Avg Loss: 0.3831\n",
      "Epoch 677, Avg Loss: 0.3836\n",
      "Epoch 678, Avg Loss: 0.3804\n",
      "Epoch 679, Avg Loss: 0.3832\n",
      "Epoch 680, Avg Loss: 0.3844\n",
      "Epoch 681, Avg Loss: 0.3852\n",
      "Epoch 682, Avg Loss: 0.3830\n",
      "Epoch 683, Avg Loss: 0.3833\n",
      "Epoch 684, Avg Loss: 0.3861\n",
      "Epoch 685, Avg Loss: 0.3874\n",
      "Epoch 686, Avg Loss: 0.3832\n",
      "Epoch 687, Avg Loss: 0.3805\n",
      "Epoch 688, Avg Loss: 0.3837\n",
      "Epoch 689, Avg Loss: 0.3872\n",
      "Epoch 690, Avg Loss: 0.3894\n",
      "Epoch 691, Avg Loss: 0.3874\n",
      "Epoch 692, Avg Loss: 0.3852\n",
      "Epoch 693, Avg Loss: 0.3845\n",
      "Epoch 694, Avg Loss: 0.3838\n",
      "Epoch 695, Avg Loss: 0.3816\n",
      "Epoch 696, Avg Loss: 0.3915\n",
      "Epoch 697, Avg Loss: 0.3797\n",
      "Epoch 698, Avg Loss: 0.3868\n",
      "Epoch 699, Avg Loss: 0.3875\n",
      "Epoch 700, Avg Loss: 0.3791\n",
      "Epoch 701, Avg Loss: 0.3879\n",
      "Epoch 702, Avg Loss: 0.3889\n",
      "Epoch 703, Avg Loss: 0.3853\n",
      "Epoch 704, Avg Loss: 0.3797\n",
      "Epoch 705, Avg Loss: 0.3816\n",
      "Epoch 706, Avg Loss: 0.3810\n",
      "Epoch 707, Avg Loss: 0.3801\n",
      "Epoch 708, Avg Loss: 0.3804\n",
      "Epoch 709, Avg Loss: 0.3804\n",
      "Epoch 710, Avg Loss: 0.3879\n",
      "Epoch 711, Avg Loss: 0.3839\n",
      "Epoch 712, Avg Loss: 0.3805\n",
      "Epoch 713, Avg Loss: 0.3822\n",
      "Epoch 714, Avg Loss: 0.3812\n",
      "Epoch 715, Avg Loss: 0.3790\n",
      "Epoch 716, Avg Loss: 0.3857\n",
      "Epoch 717, Avg Loss: 0.3882\n",
      "Epoch 718, Avg Loss: 0.3907\n",
      "Epoch 719, Avg Loss: 0.3818\n",
      "Epoch 720, Avg Loss: 0.3849\n",
      "Epoch 721, Avg Loss: 0.3911\n",
      "Epoch 722, Avg Loss: 0.3844\n",
      "Epoch 723, Avg Loss: 0.3790\n",
      "Epoch 724, Avg Loss: 0.3783\n",
      "Epoch 725, Avg Loss: 0.3817\n",
      "Epoch 726, Avg Loss: 0.3771\n",
      "Epoch 727, Avg Loss: 0.3839\n",
      "Epoch 728, Avg Loss: 0.3831\n",
      "Epoch 729, Avg Loss: 0.3834\n",
      "Epoch 730, Avg Loss: 0.3769\n",
      "Epoch 731, Avg Loss: 0.3827\n",
      "Epoch 732, Avg Loss: 0.3791\n",
      "Epoch 733, Avg Loss: 0.3788\n",
      "Epoch 734, Avg Loss: 0.3815\n",
      "Epoch 735, Avg Loss: 0.3799\n",
      "Epoch 736, Avg Loss: 0.3794\n",
      "Epoch 737, Avg Loss: 0.3774\n",
      "Epoch 738, Avg Loss: 0.3845\n",
      "Epoch 739, Avg Loss: 0.3868\n",
      "Epoch 740, Avg Loss: 0.3859\n",
      "Epoch 741, Avg Loss: 0.3820\n",
      "Epoch 742, Avg Loss: 0.3811\n",
      "Epoch 743, Avg Loss: 0.3812\n",
      "Epoch 744, Avg Loss: 0.3785\n",
      "Epoch 745, Avg Loss: 0.4068\n",
      "Epoch 746, Avg Loss: 0.3855\n",
      "Epoch 747, Avg Loss: 0.3809\n",
      "Epoch 748, Avg Loss: 0.3826\n",
      "Epoch 749, Avg Loss: 0.3753\n",
      "Epoch 750, Avg Loss: 0.3782\n",
      "Epoch 751, Avg Loss: 0.3766\n",
      "Epoch 752, Avg Loss: 0.3796\n",
      "Epoch 753, Avg Loss: 0.3835\n",
      "Epoch 754, Avg Loss: 0.3787\n",
      "Epoch 755, Avg Loss: 0.3798\n",
      "Epoch 756, Avg Loss: 0.3794\n",
      "Epoch 757, Avg Loss: 0.3731\n",
      "Epoch 758, Avg Loss: 0.3776\n",
      "Epoch 759, Avg Loss: 0.3797\n",
      "Epoch 760, Avg Loss: 0.3779\n",
      "Epoch 761, Avg Loss: 0.3773\n",
      "Epoch 762, Avg Loss: 0.3822\n",
      "Epoch 763, Avg Loss: 0.3892\n",
      "Epoch 764, Avg Loss: 0.3902\n",
      "Epoch 765, Avg Loss: 0.3854\n",
      "Epoch 766, Avg Loss: 0.3845\n",
      "Epoch 767, Avg Loss: 0.3770\n",
      "Epoch 768, Avg Loss: 0.3781\n",
      "Epoch 769, Avg Loss: 0.3748\n",
      "Epoch 770, Avg Loss: 0.3719\n",
      "Epoch 771, Avg Loss: 0.3817\n",
      "Epoch 772, Avg Loss: 0.3739\n",
      "Epoch 773, Avg Loss: 0.3750\n",
      "Epoch 774, Avg Loss: 0.3799\n",
      "Epoch 775, Avg Loss: 0.3745\n",
      "Epoch 776, Avg Loss: 0.3750\n",
      "Epoch 777, Avg Loss: 0.3783\n",
      "Epoch 778, Avg Loss: 0.3770\n",
      "Epoch 779, Avg Loss: 0.3771\n",
      "Epoch 780, Avg Loss: 0.3766\n",
      "Epoch 781, Avg Loss: 0.3812\n",
      "Epoch 782, Avg Loss: 0.3792\n",
      "Epoch 783, Avg Loss: 0.3748\n",
      "Epoch 784, Avg Loss: 0.3787\n",
      "Epoch 785, Avg Loss: 0.3760\n",
      "Epoch 786, Avg Loss: 0.3743\n",
      "Epoch 787, Avg Loss: 0.3795\n",
      "Epoch 788, Avg Loss: 0.3728\n",
      "Epoch 789, Avg Loss: 0.3843\n",
      "Epoch 790, Avg Loss: 0.3860\n",
      "Epoch 791, Avg Loss: 0.4212\n",
      "Epoch 792, Avg Loss: 0.4072\n",
      "Epoch 793, Avg Loss: 0.3975\n",
      "Epoch 794, Avg Loss: 0.3865\n",
      "Epoch 795, Avg Loss: 0.3857\n",
      "Epoch 796, Avg Loss: 0.3861\n",
      "Epoch 797, Avg Loss: 0.3831\n",
      "Epoch 798, Avg Loss: 0.3831\n",
      "Epoch 799, Avg Loss: 0.3792\n",
      "Epoch 800, Avg Loss: 0.3796\n",
      "Epoch 801, Avg Loss: 0.3812\n",
      "Epoch 802, Avg Loss: 0.3807\n",
      "Epoch 803, Avg Loss: 0.3726\n",
      "Epoch 804, Avg Loss: 0.3738\n",
      "Epoch 805, Avg Loss: 0.3776\n",
      "Epoch 806, Avg Loss: 0.3759\n",
      "Epoch 807, Avg Loss: 0.3750\n",
      "Epoch 808, Avg Loss: 0.3759\n",
      "Epoch 809, Avg Loss: 0.3791\n",
      "Epoch 810, Avg Loss: 0.3732\n",
      "Epoch 811, Avg Loss: 0.3739\n",
      "Epoch 812, Avg Loss: 0.3719\n",
      "Epoch 813, Avg Loss: 0.3766\n",
      "Epoch 814, Avg Loss: 0.3779\n",
      "Epoch 815, Avg Loss: 0.3780\n",
      "Epoch 816, Avg Loss: 0.3803\n",
      "Epoch 817, Avg Loss: 0.3734\n",
      "Epoch 818, Avg Loss: 0.3718\n",
      "Epoch 819, Avg Loss: 0.3690\n",
      "Epoch 820, Avg Loss: 0.3761\n",
      "Epoch 821, Avg Loss: 0.3765\n",
      "Epoch 822, Avg Loss: 0.3735\n",
      "Epoch 823, Avg Loss: 0.3784\n",
      "Epoch 824, Avg Loss: 0.3715\n",
      "Epoch 825, Avg Loss: 0.3732\n",
      "Epoch 826, Avg Loss: 0.3768\n",
      "Epoch 827, Avg Loss: 0.3724\n",
      "Epoch 828, Avg Loss: 0.3805\n",
      "Epoch 829, Avg Loss: 0.3699\n",
      "Epoch 830, Avg Loss: 0.3676\n",
      "Epoch 831, Avg Loss: 0.3742\n",
      "Epoch 832, Avg Loss: 0.3751\n",
      "Epoch 833, Avg Loss: 0.3733\n",
      "Epoch 834, Avg Loss: 0.3743\n",
      "Epoch 835, Avg Loss: 0.3879\n",
      "Epoch 836, Avg Loss: 0.3760\n",
      "Epoch 837, Avg Loss: 0.3715\n",
      "Epoch 838, Avg Loss: 0.3727\n",
      "Epoch 839, Avg Loss: 0.3773\n",
      "Epoch 840, Avg Loss: 0.3762\n",
      "Epoch 841, Avg Loss: 0.3688\n",
      "Epoch 842, Avg Loss: 0.3785\n",
      "Epoch 843, Avg Loss: 0.3760\n",
      "Epoch 844, Avg Loss: 0.3687\n",
      "Epoch 845, Avg Loss: 0.3696\n",
      "Epoch 846, Avg Loss: 0.3712\n",
      "Epoch 847, Avg Loss: 0.3746\n",
      "Epoch 848, Avg Loss: 0.3791\n",
      "Epoch 849, Avg Loss: 0.3884\n",
      "Epoch 850, Avg Loss: 0.3996\n",
      "Epoch 851, Avg Loss: 0.3762\n",
      "Epoch 852, Avg Loss: 0.3739\n",
      "Epoch 853, Avg Loss: 0.3741\n",
      "Epoch 854, Avg Loss: 0.3758\n",
      "Epoch 855, Avg Loss: 0.3750\n",
      "Epoch 856, Avg Loss: 0.3723\n",
      "Epoch 857, Avg Loss: 0.3750\n",
      "Epoch 858, Avg Loss: 0.3733\n",
      "Epoch 859, Avg Loss: 0.3679\n",
      "Epoch 860, Avg Loss: 0.3767\n",
      "Epoch 861, Avg Loss: 0.3821\n",
      "Epoch 862, Avg Loss: 0.3800\n",
      "Epoch 863, Avg Loss: 0.3840\n",
      "Epoch 864, Avg Loss: 0.3740\n",
      "Epoch 865, Avg Loss: 0.3733\n",
      "Epoch 866, Avg Loss: 0.3706\n",
      "Epoch 867, Avg Loss: 0.3705\n",
      "Epoch 868, Avg Loss: 0.3738\n",
      "Epoch 869, Avg Loss: 0.3703\n",
      "Epoch 870, Avg Loss: 0.3673\n",
      "Epoch 871, Avg Loss: 0.3737\n",
      "Epoch 872, Avg Loss: 0.3788\n",
      "Epoch 873, Avg Loss: 0.3733\n",
      "Epoch 874, Avg Loss: 0.3678\n",
      "Epoch 875, Avg Loss: 0.3706\n",
      "Epoch 876, Avg Loss: 0.3785\n",
      "Epoch 877, Avg Loss: 0.3707\n",
      "Epoch 878, Avg Loss: 0.3738\n",
      "Epoch 879, Avg Loss: 0.3693\n",
      "Epoch 880, Avg Loss: 0.3732\n",
      "Epoch 881, Avg Loss: 0.3787\n",
      "Epoch 882, Avg Loss: 0.3706\n",
      "Epoch 883, Avg Loss: 0.3688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 884, Avg Loss: 0.3734\n",
      "Epoch 885, Avg Loss: 0.3746\n",
      "Epoch 886, Avg Loss: 0.3726\n",
      "Epoch 887, Avg Loss: 0.3748\n",
      "Epoch 888, Avg Loss: 0.3693\n",
      "Epoch 889, Avg Loss: 0.3730\n",
      "Epoch 890, Avg Loss: 0.3829\n",
      "Epoch 891, Avg Loss: 0.3688\n",
      "Epoch 892, Avg Loss: 0.3793\n",
      "Epoch 893, Avg Loss: 0.3750\n",
      "Epoch 894, Avg Loss: 0.3713\n",
      "Epoch 895, Avg Loss: 0.3708\n",
      "Epoch 896, Avg Loss: 0.3713\n",
      "Epoch 897, Avg Loss: 0.3716\n",
      "Epoch 898, Avg Loss: 0.3739\n",
      "Epoch 899, Avg Loss: 0.3780\n",
      "Epoch 900, Avg Loss: 0.3746\n",
      "Epoch 901, Avg Loss: 0.3923\n",
      "Epoch 902, Avg Loss: 0.3861\n",
      "Epoch 903, Avg Loss: 0.3708\n",
      "Epoch 904, Avg Loss: 0.3689\n",
      "Epoch 905, Avg Loss: 0.3698\n",
      "Epoch 906, Avg Loss: 0.3697\n",
      "Epoch 907, Avg Loss: 0.3726\n",
      "Epoch 908, Avg Loss: 0.3733\n",
      "Epoch 909, Avg Loss: 0.3769\n",
      "Epoch 910, Avg Loss: 0.3793\n",
      "Epoch 911, Avg Loss: 0.3729\n",
      "Epoch 912, Avg Loss: 0.3708\n",
      "Epoch 913, Avg Loss: 0.3704\n",
      "Epoch 914, Avg Loss: 0.3718\n",
      "Epoch 915, Avg Loss: 0.3754\n",
      "Epoch 916, Avg Loss: 0.3706\n",
      "Epoch 917, Avg Loss: 0.3703\n",
      "Epoch 918, Avg Loss: 0.3685\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        batch = batch.transpose(0, 1).to(device)  # (seq_len, batch_size)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_seq = batch[:seq_len//2, :].to(device)  # First half (input)\n",
    "        target_seq = batch[seq_len//2:].to(device)  # Second half (output)\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(input_seq).to(device)  # (seq_len//2, batch_size, vocab_size)\n",
    "\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(output.view(-1, 2), target_seq.reshape(-1))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Avg Loss: {total_loss / len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3454109c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sequence: [1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "def generate_palindrome(model, start_token, seq_len=20):\n",
    "    model.eval()\n",
    "    generated = torch.tensor([[start_token]], device=device)  # Start sequence\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(seq_len - 1):\n",
    "            output = model(generated)  \n",
    "            next_token = output.argmax(dim=-1)[-1, :]  \n",
    "            generated = torch.cat([generated, next_token.unsqueeze(0)], dim=0)  \n",
    "\n",
    "    return generated.squeeze().tolist()  \n",
    "\n",
    "start_token = 1  \n",
    "gen_seq = generate_palindrome(model, start_token, seq_len=20)\n",
    "print(\"Generated Sequence:\", gen_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ec5a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Sample 2: [1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Sample 3: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Sample 4: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Sample 5: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Sample 6: [1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Sample 7: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Sample 8: [1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Sample 9: [1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Sample 10: [1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    start_token = torch.randint(0, 2, (1,)).item()  \n",
    "    generated_seq = generate_palindrome(model, start_token, seq_len=20)\n",
    "    print(f\"Sample {i+1}: {generated_seq}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25de5f87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:micromamba-dem]",
   "language": "python",
   "name": "conda-env-micromamba-dem-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
